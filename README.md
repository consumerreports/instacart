# Consumer Reports and Groundwork Collaborative data analysis of Instacart's price experiments

**Key Findings**

To determine if Instacart is experimenting with pricing, and if so, just how costly it is for shoppers, Groundwork Collaborative, Consumer Reports, and More Perfect Union conducted an independent experiment involving 437 shoppers in live tests across four cities. Researchers assisted shoppers in simultaneously adding items from a specific grocery store to their Instacart shopping carts, but they stopped short of making the purchase. Researchers measured the prices displayed to each shopper and how much those prices varied from one shopper to the next. 

The results demonstrate how companies could quietly and opaquely charge different customers different prices for the same groceries:

Almost three quarters (74%) of grocery items in the experiment were offered to shoppers at multiple price points on Instacart. The platform offered as many as five different sales prices for the exact same grocery item, in the exact same store, at the exact same time. A dozen Lucerne eggs sold for $3.99, $4.28, $4.59, $4.69, and $4.79 on Instacart at a Safeway store in Washington, D.C. A box of Clif Chocolate Chip Energy bars (10 count) sold for $19.43, $19.99, and $21.99 on Instacart at a Safeway store in Seattle.

Of those items that we found Instacart experimented on, the average difference between the lowest and highest prices was 13%. Some shoppers found grocery prices that were up to 23% higher than prices available to other shoppers for the exact same items, in the exact same store, at the exact same time. A box of Signature SELECT Corn Flakes (18 ounce) on Instacart from a Safeway store in Washington, D.C., sold for $2.99, $3.49, and $3.69, with the highest price 23% greater than the lowest one.

Researchers found that overall Instacart basket totals varied by an average of about 7% for the exact same items from the exact same locations, at the exact same time. The exact same basket of groceries on Instacart from a Safeway store in Seattle, cost some shoppers $114.34, while other shoppers were shown $119.85 and $123.93. At a Target store in North Canton, Ohio, shoppers were shown different prices — $84.43, $84.81, $84.92, $87.91, and $90.47 — for the same basket of groceries.

**Methodology**

Across five separate tests in September 2025 involving 437 participants, researchers studied the prices of a basket of goods offered at five grocery stores on Instacart: two Target stores and three Safeway stores. For these tests (four were conducted online; one was conducted in-person), shoppers selected a set basket of items and captured screenshots of the prices they found in real-time.

We also performed an additional test to determine which grocery retailers showed evidence of price fluctuations via the Instacart app. This confirmation test, conducted with 88 volunteers in November 2025, evaluated the prices for two products at 15 different retailers and found evidence of similar price experimentation at Albertsons, Costco, Kroger, and Sprouts Farmers Market. This confirmation test does not inform the final calculations, but the results of this final test underscore the price variability evident throughout this study.

**Participant Recruitment and Data Collection**

Researchers recruited shoppers from a base of active Consumer Reports members, emailing consumers who had previously participated in at least one community research project with the organization. The team also recruited participants more widely, using Consumer Reports’ social media channels and members from a nonprofit journalism organization, More Perfect Union.

Participants signed up for one of four different Consumer Reports-hosted tests, which were conducted remotely through online video conferencing software in September 2025. The tests focused on four stores in Seattle; Washington, D.C.; Saint Paul, Minnesota; and North Canton, Ohio.

For these tests, shoppers joined online video conferences where they were instructed on how to create an account for Instacart, if they didn’t have one; enter a particular store address; and select the option for in-store pick-up. (To eliminate concerns about how delivery services might impact item costs or basket totals, this study focused on grocery orders that could be picked up at the store, rather than delivered to a home address.) Then, researchers helped shoppers find and add between 18 and 20 specific items to their shopping carts. At the end of each hour-long test, participants were asked to capture screenshots of the prices they found in real-time, submit those screenshots, and complete a survey about their basic demographics and shopping history.

In addition, researchers recruited in-person study participants with the help of a local nonprofit to conduct a similar shopping test of the Instacart platform for a Safeway store in Washington, D.C. For this test, which was co-hosted by Groundwork Collaborative and More Perfect Union, researchers and participants gathered in a conference room about a mile from the store and simultaneously used their phones or other devices to shop online for groceries. In this case, the geolocation of participants was not a variable. The participants were physically close to one another and followed the same method of shopping on the Instacart platform as in the previous virtual tests. In the end, the results of the conference room test were indistinguishable from the findings of the video conferenced tests.

A total of 437 people from across the U.S. reported their Instacart pricing data, and after removing incomplete or incorrect files, 193 of those submissions were cleaned, processed, and included in the analyses.

**Data Analysis**

The research team processed up to eight screenshots for each of the 193 participants (179 online and 14 in-person) by first confirming that the screenshots showed the correct store and location and the correct products. Then, researchers entered sales prices, original prices3 and total basket prices into a database and merged them with participants’ survey responses, which included demographics and shopping history. The team did not collect information about taxes or service fees associated with any Instacart order.

To evaluate whether prices differed among participants for the same items or basket totals, we measured the prevalence and size of price variations. When price variation was present, the first step was to identify its nature. For each of the five shopping tests we conducted, we computed (a) the difference in total basket prices across participants; (b) the number of total basket prices observed; and (c) the difference between the maximum and minimum basket totals. We then identified how many distinct price tiers existed and how participants were distributed across them. In addition, we repeated this step for each item in each test. We computed (a) the spread of sales prices across participants; (b) the number of unique sales prices observed; and (c) the difference between the maximum and minimum prices.

In cases of price variation, we sought to understand whether the data we had collected about individual shoppers could explain the differences. To assess whether shopper characteristics such as age, race, gender, and income could explain differences in total basket prices, we compared average sales prices at both the basket level and the item level between groups of shoppers defined by frequency of Instacart use, age, race, gender, and income. We completed tests on group means to assess whether any observed differences were likely to be real or could simply reflect random variation.

We also used regression models at both the basket and item levels. At the basket level, the models tested whether shopper characteristics predicted differences in the total price of the basket. At the item level, the models included product fixed effects so that the comparisons were between shoppers looking at the exact same items. Two sets of regressions were estimated. In the first, we included only Instacart shopping history measured in two ways: (a) as a single binary variable for “ever used” versus “never used,” and (b) as two binary variables for “used sometimes” and “used often,” with the “never used” category omitted. This strategy allowed us to test whether prior experience with the platform alone explained price variations. In the second set of models, we added demographic controls (age, income, gender, and race) to assess whether these characteristics, when considered together, influenced basket or item prices. This stepwise approach distinguished the regression analyses from the t-tests. While t-tests compared average prices between individual groups, the regressions assessed whether any observed differences persist after controlling for multiple factors simultaneously. Across both model specifications, the differences associated with Instacart use and demographics were small and not statistically meaningful.

Our participant pool is not representative of the U.S. population. But this study’s findings are robust and offer a meaningful contribution to understanding how new digital technologies are being used to limit pricing transparency and stability for consumers.
